{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Building a Search Engine for Educational Content\n",
                "## CSCI S-89B Final Project - Seymur Hasanov\n",
                "**Harvard Extension School | Fall 2025**\n",
                "\n",
                "This notebook demonstrates the complete NLP pipeline for analyzing academic research papers.\n",
                "\n",
                "### Components:\n",
                "1. **LDA Topic Modeling** - Discover hidden themes in papers\n",
                "2. **Sentence Transformers** - Semantic search with BERT embeddings\n",
                "3. **Neural Network Classifier** - Topic prediction with Keras/TensorFlow\n",
                "4. **t-SNE Visualization** - Embedding visualization\n",
                "5. **Classical ML Comparison** - Baseline comparison\n",
                "6. **Interactive Demo** - Streamlit web application\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 1: Setup and Installation"
            ],
            "metadata": {
                "id": "step1_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Install required packages\n",
                "!pip install -q streamlit gensim>=4.4.0 sentence-transformers tensorflow pyLDAvis wordcloud arxiv nltk plotly scikit-learn\n",
                "\n",
                "# Download NLTK data\n",
                "import nltk\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('wordnet', quiet=True)\n",
                "\n",
                "print(\"‚úÖ All dependencies installed!\")"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 2: Clone Repository from GitHub"
            ],
            "metadata": {
                "id": "step2_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Clone the project repository\n",
                "!git clone https://github.com/Seymurhh/Search_engine_educational_project_NLP.git\n",
                "%cd Search_engine_educational_project_NLP\n",
                "\n",
                "print(\"‚úÖ Repository cloned successfully!\")"
            ],
            "metadata": {
                "id": "clone"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 3: Import Modules and Load Data"
            ],
            "metadata": {
                "id": "step3_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Import project modules\n",
                "import data_loader\n",
                "import topic_model\n",
                "import semantic_search\n",
                "import neural_classifier\n",
                "\n",
                "print(\"‚úÖ Modules imported!\")\n",
                "\n",
                "# Load dataset\n",
                "print(\"\\nüìä Loading dataset...\")\n",
                "df = data_loader.load_from_csv(\"arxiv_dataset.csv\")\n",
                "print(f\"‚úÖ Loaded {len(df)} papers from ArXiv\")\n",
                "print(f\"   Categories: cs.RO (Robotics), cs.AI (Artificial Intelligence)\")\n",
                "\n",
                "# Show sample\n",
                "print(\"\\nüìÑ Sample Paper Titles:\")\n",
                "for i, title in enumerate(df['title'].head(3)):\n",
                "    print(f\"   {i+1}. {title[:80]}...\")"
            ],
            "metadata": {
                "id": "import_load"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 4: Text Preprocessing"
            ],
            "metadata": {
                "id": "step4_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"üîß Preprocessing text...\")\n",
                "processed_docs = [data_loader.preprocess_text(doc) for doc in df['abstract']]\n",
                "processed_docs = data_loader.make_bigrams(processed_docs)\n",
                "\n",
                "print(f\"‚úÖ Preprocessed {len(processed_docs)} documents\")\n",
                "print(f\"\\nüìù Sample preprocessed tokens (first document):\")\n",
                "print(f\"   {processed_docs[0][:10]}...\")"
            ],
            "metadata": {
                "id": "preprocess"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 5: Topic Modeling with LDA"
            ],
            "metadata": {
                "id": "step5_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create dictionary and corpus\n",
                "print(\"üìö Creating dictionary and corpus...\")\n",
                "dictionary, corpus = topic_model.create_dictionary_corpus(processed_docs)\n",
                "print(f\"   Dictionary size: {len(dictionary)} unique terms\")\n",
                "\n",
                "# Train LDA model with 5 topics\n",
                "NUM_TOPICS = 5\n",
                "print(f\"\\nüéØ Training LDA model with {NUM_TOPICS} topics...\")\n",
                "lda_model = topic_model.train_lda_model(corpus, dictionary, num_topics=NUM_TOPICS)\n",
                "\n",
                "# Compute coherence score\n",
                "print(\"\\nüìà Computing coherence score...\")\n",
                "coherence_score = topic_model.compute_coherence_score(lda_model, processed_docs, dictionary)\n",
                "print(f\"\\n\" + \"=\"*60)\n",
                "print(f\"üéØ COHERENCE SCORE (Cv): {coherence_score:.4f}\")\n",
                "print(f\"   (Scores > 0.4 are considered acceptable)\")\n",
                "print(\"=\"*60)"
            ],
            "metadata": {
                "id": "lda_train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 5.1 Discovered Topics"
            ],
            "metadata": {
                "id": "topics_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"\\nüìä DISCOVERED TOPICS:\")\n",
                "print(\"-\" * 70)\n",
                "topics = topic_model.get_topics(lda_model, num_words=10)\n",
                "topic_names = [\n",
                "    \"LLM Reasoning & Agents\",\n",
                "    \"General ML/AI Methods\",\n",
                "    \"Reinforcement Learning\",\n",
                "    \"Dynamic Systems & Control\",\n",
                "    \"Visual Robotics & Planning\"\n",
                "]\n",
                "\n",
                "for idx, topic in topics:\n",
                "    words = [word.split('*')[1].strip().strip('\"') for word in topic.split(' + ')]\n",
                "    print(f\"\\nTopic {idx} ({topic_names[idx]}):\")\n",
                "    print(f\"   Keywords: {', '.join(words[:8])}\")"
            ],
            "metadata": {
                "id": "show_topics"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 5.2 Topic Distribution"
            ],
            "metadata": {
                "id": "dist_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from collections import Counter\n",
                "\n",
                "topic_counts = Counter()\n",
                "doc_topics = []\n",
                "\n",
                "for i, doc_bow in enumerate(corpus):\n",
                "    topic_dist = lda_model.get_document_topics(doc_bow)\n",
                "    if topic_dist:\n",
                "        dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
                "        topic_counts[dominant_topic] += 1\n",
                "        doc_topics.append(dominant_topic)\n",
                "    else:\n",
                "        doc_topics.append(-1)\n",
                "\n",
                "df['dominant_topic'] = doc_topics\n",
                "\n",
                "print(\"\\nüìà TOPIC DISTRIBUTION:\")\n",
                "print(\"-\" * 50)\n",
                "for topic_id in range(NUM_TOPICS):\n",
                "    count = topic_counts.get(topic_id, 0)\n",
                "    pct = count / len(df) * 100\n",
                "    print(f\"Topic {topic_id} ({topic_names[topic_id][:20]}): {count} papers ({pct:.1f}%)\")"
            ],
            "metadata": {
                "id": "topic_dist"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 5.3 Topic Distribution Visualization"
            ],
            "metadata": {
                "id": "viz_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "topics_list = list(range(NUM_TOPICS))\n",
                "counts = [topic_counts.get(t, 0) for t in topics_list]\n",
                "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
                "\n",
                "bars = ax.bar(topics_list, counts, color=colors, edgecolor='black', linewidth=1.2)\n",
                "ax.set_xlabel('Topic ID', fontsize=12)\n",
                "ax.set_ylabel('Number of Papers', fontsize=12)\n",
                "ax.set_title('Topic Distribution Across 500 ArXiv Papers', fontsize=14, fontweight='bold')\n",
                "ax.set_xticks(topics_list)\n",
                "\n",
                "for bar, count in zip(bars, counts):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
                "            str(count), ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ],
            "metadata": {
                "id": "topic_viz"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 6: Semantic Search with Sentence Transformers"
            ],
            "metadata": {
                "id": "step6_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"\\nüîç Initializing Semantic Search...\")\n",
                "searcher = semantic_search.SemanticSearch()\n",
                "\n",
                "print(\"   Encoding 500 paper abstracts...\")\n",
                "paper_embeddings = searcher.encode_papers(tuple(df['abstract'].tolist()))\n",
                "searcher.paper_embeddings = paper_embeddings\n",
                "\n",
                "print(f\"‚úÖ Created {paper_embeddings.shape[0]} embeddings of dimension {paper_embeddings.shape[1]}\")"
            ],
            "metadata": {
                "id": "semantic_init"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 6.1 Semantic Search Demo"
            ],
            "metadata": {
                "id": "search_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "test_queries = [\n",
                "    \"reinforcement learning for robot control\",\n",
                "    \"transformer architecture for natural language\",\n",
                "    \"autonomous navigation in complex environments\"\n",
                "]\n",
                "\n",
                "print(\"\\nüîç SEMANTIC SEARCH DEMO:\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for query in test_queries:\n",
                "    print(f\"\\nüìù Query: '{query}'\")\n",
                "    print(\"-\"*70)\n",
                "    \n",
                "    results = searcher.search(query, paper_embeddings, df, top_k=3)\n",
                "    for i, result in enumerate(results):\n",
                "        title = result['title'][:65]\n",
                "        score = result['score']\n",
                "        print(f\"   {i+1}. [{score:.3f}] {title}...\")"
            ],
            "metadata": {
                "id": "search_demo"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 6.2 t-SNE Embedding Visualization"
            ],
            "metadata": {
                "id": "tsne_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from sklearn.manifold import TSNE\n",
                "\n",
                "print(\"\\nüé® Computing t-SNE projection...\")\n",
                "\n",
                "X_embed = paper_embeddings.cpu().numpy()\n",
                "y_topics = np.array(doc_topics)\n",
                "\n",
                "valid_mask = y_topics >= 0\n",
                "X_valid = X_embed[valid_mask]\n",
                "y_valid = y_topics[valid_mask]\n",
                "\n",
                "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
                "X_2d = tsne.fit_transform(X_valid)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 8))\n",
                "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
                "\n",
                "for topic_id in range(NUM_TOPICS):\n",
                "    mask = y_valid == topic_id\n",
                "    ax.scatter(X_2d[mask, 0], X_2d[mask, 1], \n",
                "               c=colors[topic_id], label=topic_names[topic_id], alpha=0.7, s=50)\n",
                "\n",
                "ax.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
                "ax.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
                "ax.set_title('t-SNE Visualization of Paper Embeddings', fontsize=14, fontweight='bold')\n",
                "ax.legend(loc='best', fontsize=9)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Papers with similar topics cluster together!\")"
            ],
            "metadata": {
                "id": "tsne_viz"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 7: Neural Network Classifier"
            ],
            "metadata": {
                "id": "step7_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"\\nüß† Training Neural Network Classifier...\")\n",
                "\n",
                "X = paper_embeddings.cpu().numpy()\n",
                "y = np.array(doc_topics)\n",
                "\n",
                "valid_mask = y >= 0\n",
                "X = X[valid_mask]\n",
                "y = y[valid_mask]\n",
                "\n",
                "print(f\"   Training samples: {len(X)}\")\n",
                "print(f\"   Number of classes: {NUM_TOPICS}\")\n",
                "print(f\"   Random baseline: {1/NUM_TOPICS:.1%}\")\n",
                "\n",
                "classifier, history = neural_classifier.train_classifier(\n",
                "    X, y, num_topics=NUM_TOPICS, epochs=30, batch_size=32\n",
                ")\n",
                "\n",
                "print(f\"\\n\" + \"=\"*60)\n",
                "print(f\"üéØ KERAS NEURAL NETWORK RESULTS:\")\n",
                "print(f\"   Training Accuracy:   {history.history['accuracy'][-1]:.1%}\")\n",
                "print(f\"   Validation Accuracy: {history.history['val_accuracy'][-1]:.1%}\")\n",
                "print(\"=\"*60)"
            ],
            "metadata": {
                "id": "train_classifier"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 7.1 Training History"
            ],
            "metadata": {
                "id": "train_viz_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "ax1.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
                "ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
                "ax1.axhline(y=1/NUM_TOPICS, color='gray', linestyle='--', label=f'Random ({1/NUM_TOPICS:.0%})')\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Accuracy')\n",
                "ax1.set_title('Model Accuracy')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "ax2.plot(history.history['loss'], label='Training', linewidth=2)\n",
                "ax2.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Loss')\n",
                "ax2.set_title('Model Loss')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ],
            "metadata": {
                "id": "train_viz"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 7.2 Classical ML Comparison"
            ],
            "metadata": {
                "id": "ml_comparison_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "print(\"\\nüìä CLASSICAL ML COMPARISON:\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "classifiers = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
                "    'SVM (RBF)': SVC(kernel='rbf'),\n",
                "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
                "}\n",
                "\n",
                "results = {'Random Baseline': 1/NUM_TOPICS}\n",
                "\n",
                "for name, clf in classifiers.items():\n",
                "    clf.fit(X_train, y_train)\n",
                "    acc = clf.score(X_test, y_test)\n",
                "    results[name] = acc\n",
                "    print(f\"   {name}: {acc:.1%}\")\n",
                "\n",
                "results['Keras Neural Network'] = history.history['val_accuracy'][-1]\n",
                "print(f\"   Keras Neural Network: {results['Keras Neural Network']:.1%}\")\n",
                "\n",
                "print(\"\\n‚úÖ All classifiers significantly outperform the 20% random baseline!\")"
            ],
            "metadata": {
                "id": "ml_comparison"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 8: Results Summary\n",
                "\n",
                "| Metric | Value |\n",
                "|--------|-------|\n",
                "| Papers | 500 |\n",
                "| Topics | 5 |\n",
                "| Coherence | 0.4170 |\n",
                "| Embedding Dim | 384 |\n",
                "| Classifier Accuracy | ~53% |\n",
                "| Random Baseline | 20% |"
            ],
            "metadata": {
                "id": "summary_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üìä FINAL RESULTS SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(f\"üìö Dataset: {len(df)} ArXiv papers\")\n",
                "print(f\"üéØ Topics: {NUM_TOPICS}\")\n",
                "print(f\"üìà Coherence: {coherence_score:.4f}\")\n",
                "print(f\"üîç Embedding: {paper_embeddings.shape[1]} dimensions\")\n",
                "print(f\"üß† Validation Accuracy: {history.history['val_accuracy'][-1]:.1%}\")\n",
                "print(f\"üìä Random Baseline: {1/NUM_TOPICS:.1%}\")\n",
                "print(\"=\"*70)\n",
                "print(\"\\n‚úÖ ALL ANALYSIS COMPLETE!\")"
            ],
            "metadata": {
                "id": "summary"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 9: Interactive Demo (Streamlit)"
            ],
            "metadata": {
                "id": "step9_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!wget -q -O cloudflared-linux-amd64 https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
                "!chmod +x cloudflared-linux-amd64\n",
                "print(\"‚úÖ Cloudflared downloaded\")"
            ],
            "metadata": {
                "id": "download_cf"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "!streamlit run app.py &>/content/logs.txt &\n",
                "print(\"üöÄ Streamlit starting...\")\n",
                "\n",
                "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:8501 > cloudflared.log 2>&1 &\n",
                "\n",
                "import time\n",
                "time.sleep(8)\n",
                "print(\"\\nüåê Public URL:\")\n",
                "!grep -o 'https://.*\\.trycloudflare\\.com' cloudflared.log || echo \"Run again if no URL.\""
            ],
            "metadata": {
                "id": "start_streamlit"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Click the URL above for the interactive demo!"
            ],
            "metadata": {
                "id": "demo_footer"
            }
        }
    ]
}